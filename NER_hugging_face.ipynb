{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#### STARTS HERE ####"
      ],
      "metadata": {
        "id": "qdT2JfAarzSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  datasets evaluate\n",
        "#!pip install transformers"
      ],
      "metadata": {
        "id": "vZJh8X_XsC8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "#!pip install transformers==4.27.3"
      ],
      "metadata": {
        "id": "DlPM1DZaRPVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"bert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "Z5sqbGdkRg7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# label_list =[\n",
        "#  'O',               #0            \n",
        "#  'B-disease',       #1        \n",
        "#  'I-disease',       #2\n",
        "#  'B-symptom',       #3\n",
        "#  'I-symptom',       #4\n",
        "#  'B-test',          #5\n",
        "#  'I-test',          #6\n",
        "#  'B-demographics',  #7\n",
        "#  'I-demographics',  #8\n",
        "#  'B-severity',      #9\n",
        "#  'I-severity',      #10\n",
        "#  'B-mental_state',  #11\n",
        "#  'I-mental_state',  #12\n",
        "#  'B-anatomical',    #13\n",
        "#  'I-anatomical',    #14\n",
        "#  'B-other_relevant',#15\n",
        "#  'I-other_relevant' #16\n",
        "#  ]\n",
        "\n",
        "label_list = [\n",
        " 'O'               ,#0           \n",
        " 'B-DISEASE'       ,#1\n",
        " 'I-DISEASE'       ,#2\n",
        " 'B-SYMPTOM'       ,#3\n",
        " 'I-SYMPTOM'       ,#4\n",
        " 'B-TEST'          ,#5\n",
        " 'I-TEST'          ,#6\n",
        " 'B-DEMOGRAPHICS'  ,#7\n",
        " 'I-DEMOGRAPHICS'  ,#8\n",
        " 'B-SEVERITY'      ,#9\n",
        " 'I-SEVERITY'      ,#10\n",
        " 'B-MENTAL_STATE'  ,#11\n",
        " 'I-MENTAL_STATE'  ,#12\n",
        " 'B-ANATOMICAL'    ,#13\n",
        " 'I-ANATOMICAL'    ,#14\n",
        " 'B-OTHER_RELEVANT',#15\n",
        " 'I-OTHER_RELEVANT',#16\n",
        " 'B-TREATMENT'     ,#17\n",
        " 'I-TREATMENT'     ,#18\n",
        "]"
      ],
      "metadata": {
        "id": "UXx44r1zSFpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {\n",
        " 0:  'O',                          \n",
        " 1:  'B-DISEASE',              \n",
        " 2:  'I-DISEASE',       \n",
        " 3:  'B-SYMPTOM',       \n",
        " 4:  'I-SYMPTOM',       \n",
        " 5:  'B-TEST',          \n",
        " 6:  'I-TEST',          \n",
        " 7:  'B-DEMOGRAPHICS',  \n",
        " 8:  'I-DEMOGRAPHICS',  \n",
        " 9:  'B-SEVERITY',      \n",
        " 10: 'I-SEVERITY',      \n",
        " 11: 'B-MENTAL_STATE',  \n",
        " 12: 'I-MENTAL_STATE',  \n",
        " 13: 'B-ANATOMICAL',    \n",
        " 14: 'I-ANATOMICAL',    \n",
        " 15: 'B-OTHER_RELEVANT',\n",
        " 16: 'I-OTHER_RELEVANT',\n",
        " 17: 'B-TREATMENT', \n",
        " 18: 'I-TREATMENT'\n",
        "}\n",
        "label2id = {\n",
        " 'O'               :0,            \n",
        " 'B-DISEASE'       :1,        \n",
        " 'I-DISEASE'       :2,\n",
        " 'B-SYMPTOM'       :3,\n",
        " 'I-SYMPTOM'       :4,\n",
        " 'B-TEST'          :5,\n",
        " 'I-TEST'          :6,\n",
        " 'B-DEMOGRAPHICS'  :7,\n",
        " 'I-DEMOGRAPHICS'  :8,\n",
        " 'B-SEVERITY'      :9,\n",
        " 'I-SEVERITY'      :10,\n",
        " 'B-MENTAL_STATE'  :11,\n",
        " 'I-MENTAL_STATE'  :12,\n",
        " 'B-ANATOMICAL'    :13,\n",
        " 'I-ANATOMICAL'    :14,\n",
        " 'B-OTHER_RELEVANT':15,\n",
        " 'I-OTHER_RELEVANT':16,\n",
        " 'B-TREATMENT'     :17,\n",
        " 'I-TREATMENT'     :18\n",
        "}"
      ],
      "metadata": {
        "id": "oQEFJB5rR_1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "jM9O-t7XWeBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize\n",
        "import re\n",
        "#PATH = \"gdrive/MyDrive/lab_project_with_Tom_Hope/annotation/evals/teamtat_annotated_data_v2/\"\n",
        "PATH = \"gdrive/MyDrive/lab_project_with_Tom_Hope/annotation/evals/teamtat_annotated_data_v3/\"\n",
        "\n",
        "files = os.listdir(PATH)\n",
        "\n",
        "def get_token_and_annotations_empty(start, end, text):\n",
        "    sub_text = text[start:end]\n",
        "    tokens = word_tokenize(sub_text)\n",
        "    annotations = ['O']*len(tokens)\n",
        "    return tokens, annotations\n",
        "\n",
        "\n",
        "def get_token_and_annotations_with_annotation(start, end, text, tag):\n",
        "    sub_text = text[start:end]\n",
        "    tokens = word_tokenize(sub_text)\n",
        "    annotations = [f\"B-{tag}\"]\n",
        "    for i in range(1, len(tokens)):\n",
        "        annotations.append(f\"I-{tag}\")\n",
        "    return tokens, annotations\n",
        "\n",
        "def get_token_and_annotations(place_in_text_and_annotation, text):\n",
        "    tokens, annotations = [], []\n",
        "    start = 0\n",
        "    for place, tag in place_in_text_and_annotation.items():\n",
        "        annotation_start, annotation_end = place\n",
        "        #start, annotation_start\n",
        "        tokens_empty, annotations_empty = get_token_and_annotations_empty(start, annotation_start, text)\n",
        "        tokens += tokens_empty\n",
        "        annotations += annotations_empty\n",
        "        tokens_tag, annotations_tag = get_token_and_annotations_with_annotation(annotation_start, annotation_end, text, tag)\n",
        "        tokens += tokens_tag\n",
        "        annotations += annotations_tag\n",
        "        start += annotation_end\n",
        "    tokens_empty, annotations_empty = get_token_and_annotations_empty(start, len(text), text)\n",
        "    tokens += tokens_empty\n",
        "    annotations += annotations_empty\n",
        "    return tokens, annotations\n",
        "\n",
        "def preprocess_file(file_name):\n",
        "    \"\"\"\n",
        "    gets an annotated file either a post_<num>@, or comment_<num>@ and\n",
        "    :param file_name: one annotated post/comment.\n",
        "    :return: 2 lists of the tokens and the matching annotations\n",
        "    \"\"\"\n",
        "    tokens, annotations = [], []\n",
        "    with open(f\"{PATH}/{file_name}\", 'r') as f:\n",
        "        data = f.read()\n",
        "    Bs_data = BeautifulSoup(data, \"xml\")\n",
        "    #text = Bs_data.find('text').text\n",
        "    passages = Bs_data.find_all('passage')\n",
        "    for passage in passages:\n",
        "        place_in_text_and_annotation = {}\n",
        "        b_annotations = passage.find_all('annotation')\n",
        "        text = passage.find('text').text\n",
        "        for annotation_info in b_annotations:\n",
        "            annotation_text = annotation_info.contents[11].text\n",
        "            s = re.search(annotation_text.replace('(','').replace(')',''), text)\n",
        "            #place_in_text_and_annotation[(s.start(), s.end())] = annotation_info.infon.text\n",
        "            try:\n",
        "              place_in_text_and_annotation[(s.start(), s.end())] = annotation_info.infon.text\n",
        "            except:\n",
        "              print(f\"annotation_info.infon.text:{annotation_info.infon.text}\")\n",
        "        print(place_in_text_and_annotation)\n",
        "        tokens_pas, annotations_pas = get_token_and_annotations(place_in_text_and_annotation, text)\n",
        "        tokens += tokens_pas\n",
        "        annotations += annotations_pas\n",
        "    return tokens, annotations\n",
        "\n",
        "\n",
        "all_tokens, all_annotations = [], []\n",
        "for file_name in files:\n",
        "    cur_tokens, cur_annotations = preprocess_file(file_name)\n",
        "    all_tokens.append(cur_tokens)\n",
        "    all_annotations.append(cur_annotations)"
      ],
      "metadata": {
        "id": "MXQrVHSC0g8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new_train = []\n",
        "# new_test = []\n",
        "# id = 0\n",
        "# for i in range(0, int(len(all_tokens)*0.8)):\n",
        "#     #element = {'id': f\"{id}\"}\n",
        "#     #id += 1\n",
        "#     element = {}\n",
        "#     tokens, ner_tags = all_tokens[i],all_annotations[i]\n",
        "#     numeric_ner_tags = [label2id[tag] for tag in ner_tags]\n",
        "#     element['tokens'] = tokens\n",
        "#     element['ner_tags'] = numeric_ner_tags\n",
        "#     new_train.append(element)\n",
        "\n",
        "# id = 0\n",
        "# for i in range(int(len(all_tokens)*0.8), len(all_tokens)):\n",
        "#     #element = {'id': f\"{id}\"}\n",
        "#     #id += 1\n",
        "#     element = {}\n",
        "#     tokens, ner_tags = all_tokens[i],all_annotations[i]\n",
        "#     numeric_ner_tags = [label2id[tag] for tag in ner_tags]\n",
        "#     element['tokens'] = tokens\n",
        "#     element['ner_tags'] = numeric_ner_tags\n",
        "#     new_test.append(element)\n",
        "# new_data = {'train': new_train, 'test': new_test}\n",
        "# new_data_train = {'train': new_train}\n",
        "# new_data_test = {'validation': new_test}"
      ],
      "metadata": {
        "id": "Sm67bPrX1dlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_train = []\n",
        "new_test = []\n",
        "\n",
        "tokens_train = []\n",
        "ner_tags_train = []\n",
        "for i in range(0, int(len(all_tokens)*0.8)):\n",
        "    tokens, ner_tags = all_tokens[i],all_annotations[i]\n",
        "    numeric_ner_tags = [label2id[tag] for tag in ner_tags]\n",
        "    tokens_train.append(tokens)\n",
        "    ner_tags_train.append(numeric_ner_tags)\n",
        "\n",
        "tokens_test = []\n",
        "ner_tags_test = []\n",
        "for i in range(int(len(all_tokens)*0.8), len(all_tokens)):\n",
        "    #element = {'id': f\"{id}\"}\n",
        "    #id += 1\n",
        "    element = {}\n",
        "    tokens, ner_tags = all_tokens[i],all_annotations[i]\n",
        "    numeric_ner_tags = [label2id[tag] for tag in ner_tags]\n",
        "    tokens_test.append(tokens)\n",
        "    ner_tags_test.append(numeric_ner_tags)\n",
        "\n",
        "\n",
        "#new_data = {'train': new_train, 'test': new_test}\n",
        "#new_data_train = {'train': new_train}\n",
        "#new_data_test = {'validation': new_test}"
      ],
      "metadata": {
        "id": "b9F9ynXLPL4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "new_data_test_df = pd.DataFrame({'tokens':tokens_train, 'ner_tags':ner_tags_train})#.astype(str)\n",
        "new_data_train_df = pd.DataFrame({'tokens':tokens_test, 'ner_tags':ner_tags_test})#.astype(str)"
      ],
      "metadata": {
        "id": "Vem8M1GFH11O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import tensorflow as tf\n",
        "from datasets import Dataset\n",
        "new_data_test_tf = Dataset.from_pandas(new_data_test_df)\n",
        "new_data_train_tf = Dataset.from_pandas(new_data_train_df)\n",
        "#new_data_test_tf = tf.convert_to_tensor(new_data_test_df)\n",
        "#new_data_train_tf = tf.convert_to_tensor(new_data_train_df)"
      ],
      "metadata": {
        "id": "rLcY9sXcIne3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    print(examples)\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
        "                label_ids.append(label[word_idx])\n",
        "                #print(examples[f\"ner_tags\"][label])\n",
        "                #label_ids.append(examples[f\"ner_tags\"][label])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# new_data_test_tf = tf.convert_to_tensor(new_data_test_df)\n",
        "# new_data_train_tf = tf.convert_to_tensor(new_data_train_df)\n",
        "\n",
        "train_tokenized = new_data_train_tf.map(tokenize_and_align_labels, batched=True)\n",
        "test_tokenized= new_data_test_tf.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "# train_normalized = tokenize_and_align_labels(train_data_final)\n",
        "# test_normalized = tokenize_and_align_labels(test_data_final)\n",
        "# data_normalized = {'train': train_normalized, 'validation': test_normalized}"
      ],
      "metadata": {
        "id": "zlNLbfBa7EE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_tokenized) #Checking that out"
      ],
      "metadata": {
        "id": "i9FtGUSjDGSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")"
      ],
      "metadata": {
        "id": "L4AwGCIUDO5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval\n",
        "import evaluate\n",
        "\n",
        "seqeval = evaluate.load(\"seqeval\")"
      ],
      "metadata": {
        "id": "Om3FMnfxDXIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "Q6bgAzaAFW2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import create_optimizer\n",
        "\n",
        "batch_size = 16\n",
        "num_train_epochs = 3 #3\n",
        "num_train_steps = (len(train_tokenized) // batch_size) * num_train_epochs\n",
        "optimizer, lr_schedule = create_optimizer(\n",
        "    init_lr=2e-5,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=0.01,\n",
        "    num_warmup_steps=0,\n",
        ")"
      ],
      "metadata": {
        "id": "f3P0rhOsFshR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForTokenClassification\n",
        "\n",
        "access_token=\"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\"\n",
        "\n",
        "model = TFAutoModelForTokenClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=len(id2label), id2label=id2label, label2id=label2id ,  use_auth_token=access_token\n",
        ")"
      ],
      "metadata": {
        "id": "iHOvlkDaGEek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T9gDQQm8hfdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_train_set = model.prepare_tf_dataset(\n",
        "    train_tokenized,\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "tf_validation_set = model.prepare_tf_dataset(\n",
        "    test_tokenized,\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "KpG3iKV_GOZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model.compile(optimizer=optimizer)\n"
      ],
      "metadata": {
        "id": "D2I90eh3RlHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "\n",
        "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)"
      ],
      "metadata": {
        "id": "c87aq0EbRpr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "_64FAyrfAZ0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "\n",
        "push_to_hub_callback = PushToHubCallback(\n",
        "    output_dir=\"my_awesome_model\",\n",
        "    tokenizer=tokenizer,\n",
        "    #token=True\n",
        "    #token=\"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\"\n",
        ")\n",
        "callbacks = [metric_callback, push_to_hub_callback]"
      ],
      "metadata": {
        "id": "TaxA5zSQRxtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "EZzoqgwpS-bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"gdrive/MyDrive/lab_project_with_Tom_Hope/annotation/models/2019/model-2023-03-19-epochs.ckpt\"\n",
        "model.save_weights(checkpoint_path.format(epoch=0))\n"
      ],
      "metadata": {
        "id": "qyOavRBsTiQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall transformers\n",
        "!pip install transformers==4.18.0"
      ],
      "metadata": {
        "id": "RonG8ae1Gjw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "text = \"In most cases, once the thyroid nodules are big enough to cause airway or food pipe compression and makes breathing or eating very difficult, the next step is surgically removing nodules or one thyroid lobe. \\\n",
        "Since the big nodules physically displace and bend the trachea or food pipe, medicines don't help much. Surgery is usually recommended, like hemithyroidectomy (half the thyroid)\\\n",
        "Your doctor might refer you to get the nodules biopsied. And then refer you to a surgeon.\"\n",
        "\n",
        "classifier(text)"
      ],
      "metadata": {
        "id": "fmvE43Tb9eSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log = [\"Epoch 1/100\",\n",
        "\"3/3 [==============================] - 29s 5s/step - loss: 2.6169 - val_loss: 2.0447\",\n",
        "\"Epoch 2/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 1.5911 - val_loss: 1.1311\",\n",
        "\"Epoch 3/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.9362 - val_loss: 0.7555\",\n",
        "\"Epoch 4/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.6927 - val_loss: 0.6095\",\n",
        "\"Epoch 5/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.5828 - val_loss: 0.5693\",\n",
        "\"Epoch 6/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.5583 - val_loss: 0.5584\",\n",
        "\"Epoch 7/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.5639 - val_loss: 0.5520\",\n",
        "\"Epoch 8/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.5250 - val_loss: 0.5462\",\n",
        "\"Epoch 9/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.5582 - val_loss: 0.5400\",\n",
        "\"Epoch 10/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.6033 - val_loss: 0.5350\",\n",
        "\"Epoch 11/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.5740 - val_loss: 0.5333\",\n",
        "\"Epoch 12/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.5059 - val_loss: 0.5236\",\n",
        "\"Epoch 13/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.4923 - val_loss: 0.5160\",\n",
        "\"Epoch 14/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.5203 - val_loss: 0.5103\",\n",
        "\"Epoch 15/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.5267 - val_loss: 0.5032\",\n",
        "\"Epoch 16/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.5664 - val_loss: 0.4965\",\n",
        "\"Epoch 17/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.4607 - val_loss: 0.4885\",\n",
        "\"Epoch 18/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.4225 - val_loss: 0.4811\",\n",
        "\"Epoch 19/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.5117 - val_loss: 0.4732\",\n",
        "\"Epoch 20/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.4003 - val_loss: 0.4657\",\n",
        "\"Epoch 21/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.4441 - val_loss: 0.4580\",\n",
        "\"Epoch 22/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.5577 - val_loss: 0.4505\",\n",
        "\"Epoch 23/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.4515 - val_loss: 0.4447\",\n",
        "\"Epoch 24/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.5276 - val_loss: 0.4398\",\n",
        "\"Epoch 25/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.4209 - val_loss: 0.4370\",\n",
        "\"Epoch 26/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.3902 - val_loss: 0.4342\",\n",
        "\"Epoch 27/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.3385 - val_loss: 0.4307\",\n",
        "\"Epoch 28/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.3467 - val_loss: 0.4317\",\n",
        "\"Epoch 29/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.4382 - val_loss: 0.4273\",\n",
        "\"Epoch 30/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.3562 - val_loss: 0.4228\",\n",
        "\"Epoch 31/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.3496 - val_loss: 0.4227\",\n",
        "\"Epoch 32/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.4156 - val_loss: 0.4200\",\n",
        "\"Epoch 33/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.3540 - val_loss: 0.4160\",\n",
        "\"Epoch 34/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.3260 - val_loss: 0.4150\",\n",
        "\"Epoch 35/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.3601 - val_loss: 0.4137\",\n",
        "\"Epoch 36/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.3581 - val_loss: 0.4109\",\n",
        "\"Epoch 37/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.3225 - val_loss: 0.4118\",\n",
        "\"Epoch 38/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.3924 - val_loss: 0.4096\",\n",
        "\"Epoch 39/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.2824 - val_loss: 0.4090\",\n",
        "\"Epoch 40/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.3307 - val_loss: 0.4083\",\n",
        "\"Epoch 41/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.3076 - val_loss: 0.4059\",\n",
        "\"Epoch 42/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.4156 - val_loss: 0.4086\",\n",
        "\"Epoch 43/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.3207 - val_loss: 0.4044\",\n",
        "\"Epoch 44/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.3491 - val_loss: 0.4024\",\n",
        "\"Epoch 45/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.3377 - val_loss: 0.4052\",\n",
        "\"Epoch 46/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.2685 - val_loss: 0.4033\",\n",
        "\"Epoch 47/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.3285 - val_loss: 0.4014\",\n",
        "\"Epoch 48/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.3012 - val_loss: 0.4009\",\n",
        "\"Epoch 49/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2509 - val_loss: 0.3999\",\n",
        "\"Epoch 50/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.2488 - val_loss: 0.4001\",\n",
        "\"Epoch 51/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2633 - val_loss: 0.4005\",\n",
        "\"Epoch 52/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.2696 - val_loss: 0.4000\",\n",
        "\"Epoch 53/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.2510 - val_loss: 0.4011\",\n",
        "\"Epoch 54/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2902 - val_loss: 0.4043\",\n",
        "\"Epoch 55/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.2457 - val_loss: 0.4023\",\n",
        "\"Epoch 56/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.3243 - val_loss: 0.3985\",\n",
        "\"Epoch 57/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2947 - val_loss: 0.4040\",\n",
        "\"Epoch 58/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.2585 - val_loss: 0.4015\",\n",
        "\"Epoch 59/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.2535 - val_loss: 0.3970\",\n",
        "\"Epoch 60/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.2445 - val_loss: 0.4002\",\n",
        "\"Epoch 61/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2330 - val_loss: 0.4023\",\n",
        "\"Epoch 62/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.2446 - val_loss: 0.4002\",\n",
        "\"Epoch 63/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.1894 - val_loss: 0.4001\",\n",
        "\"Epoch 64/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2805 - val_loss: 0.4006\",\n",
        "\"Epoch 65/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2408 - val_loss: 0.4015\",\n",
        "\"Epoch 66/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2533 - val_loss: 0.4003\",\n",
        "\"Epoch 67/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2355 - val_loss: 0.4034\",\n",
        "\"Epoch 68/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.2986 - val_loss: 0.4016\",\n",
        "\"Epoch 69/100\",\n",
        "\"3/3 [==============================] - 4s 1s/step - loss: 0.3225 - val_loss: 0.3968\",\n",
        "\"Epoch 70/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.3213 - val_loss: 0.4000\",\n",
        "\"Epoch 71/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2699 - val_loss: 0.4051\",\n",
        "\"Epoch 72/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2760 - val_loss: 0.3986\",\n",
        "\"Epoch 73/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2590 - val_loss: 0.3991\",\n",
        "\"Epoch 74/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.2342 - val_loss: 0.4062\",\n",
        "\"Epoch 75/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2865 - val_loss: 0.4068\",\n",
        "\"Epoch 76/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.2345 - val_loss: 0.4000\",\n",
        "\"Epoch 77/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2765 - val_loss: 0.3993\",\n",
        "\"Epoch 78/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2866 - val_loss: 0.4037\",\n",
        "\"Epoch 79/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.2134 - val_loss: 0.4059\",\n",
        "\"Epoch 80/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.2194 - val_loss: 0.4047\",\n",
        "\"Epoch 81/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.1972 - val_loss: 0.4023\",\n",
        "\"Epoch 82/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2378 - val_loss: 0.4015\",\n",
        "\"Epoch 83/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2958 - val_loss: 0.4020\",\n",
        "\"Epoch 84/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2869 - val_loss: 0.4055\",\n",
        "\"Epoch 85/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2262 - val_loss: 0.4040\",\n",
        "\"Epoch 86/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2417 - val_loss: 0.4022\",\n",
        "\"Epoch 87/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.2285 - val_loss: 0.4023\",\n",
        "\"Epoch 88/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.2275 - val_loss: 0.4026\",\n",
        "\"Epoch 89/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2343 - val_loss: 0.4038\",\n",
        "\"Epoch 90/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2412 - val_loss: 0.4046\",\n",
        "\"Epoch 91/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2611 - val_loss: 0.4051\",\n",
        "\"Epoch 92/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.2249 - val_loss: 0.4047\",\n",
        "\"Epoch 93/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2752 - val_loss: 0.4040\",\n",
        "\"Epoch 94/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2113 - val_loss: 0.4032\",\n",
        "\"Epoch 95/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2301 - val_loss: 0.4031\",\n",
        "\"Epoch 96/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2162 - val_loss: 0.4033\",\n",
        "\"Epoch 97/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2427 - val_loss: 0.4037\",\n",
        "\"Epoch 98/100\",\n",
        "\"3/3 [==============================] - 5s 2s/step - loss: 0.2022 - val_loss: 0.4040\",\n",
        "\"Epoch 99/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2739 - val_loss: 0.4043\",\n",
        "\"Epoch 100/100\",\n",
        "\"3/3 [==============================] - 4s 2s/step - loss: 0.2204 - val_loss: 0.4045\"\n",
        "]\n",
        "loss, val_loss = [], []\n",
        "for line in log:\n",
        "  if \"loss\" in line:\n",
        "    tokens = line.split()\n",
        "    loss.append(float(tokens[-4]))\n",
        "    val_loss.append(float(tokens[-1]))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(loss)\n",
        "plt.plot(val_loss)\n",
        "plt.title(\"losses - version3 300 posts and comments\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.legend([\"training loss\", \"valudation loss\"])"
      ],
      "metadata": {
        "id": "gwA0VTRvq8o_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}